{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zinovyee.hub\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.layers import Embedding, SpatialDropout1D\n",
    "from keras.layers import Dense, Input, GRU, LSTM \n",
    "from keras.layers import Bidirectional, Dropout, GlobalMaxPool1D \n",
    "from keras.layers import CuDNNLSTM, CuDNNGRU, GlobalAveragePooling1D\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, TimeDistributed\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "import keras.backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import initializers as initializers, regularizers, constraints\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "from nltk import tokenize \n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble  import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import random\n",
    "\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "import argparse\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, precision_score, recall_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"source_codes_800_contracts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('Smart Contracts Classification.csv', sep=\";\")\n",
    "labels = labels[['Contract Hash', 'Name', 'Category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Token creation               463\n",
       "Token creation/Token sale     92\n",
       "Token sale                    52\n",
       "Utility                       52\n",
       "Finance                       43\n",
       "Dapp                          36\n",
       "?                             17\n",
       "Scam                          14\n",
       "Token creation/Finance        13\n",
       "Airdrop                        9\n",
       "Token creation/Airdrop         8\n",
       "Utility\\r                      1\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.Category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels['NotToken'] = -labels.Category.str.contains(\"Token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    628\n",
       "True     172\n",
       "Name: NotToken, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels['NotToken'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = labels.merge(df, how='left', left_on=\"Contract Hash\", right_on='address' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 6)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = labels_df.source_code\n",
    "y = labels_df.NotToken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 40000\n",
    "MAX_LEN      = 10000  \n",
    "EMB_DIM      = 50\n",
    "REC_UNITS    = 100\n",
    "BATCH_SIZE   = 4\n",
    "EPOCHS       = 4\n",
    "RS           = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t, X_test, y_t, y_test = train_test_split(\n",
    "         X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    0.7375\n",
       "True     0.2625\n",
       "Name: NotToken, dtype: float64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    0.796875\n",
       "True     0.203125\n",
       "Name: NotToken, dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_t.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=5, random_state=42)\n",
    "auc = []\n",
    "roc = []\n",
    "fscore_ = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer   = TfidfVectorizer(\n",
    "    sublinear_tf  = True,\n",
    "    strip_accents = 'unicode',\n",
    "    analyzer      = 'word',\n",
    "    token_pattern = r'\\w{1,}',\n",
    "    stop_words    = 'english',\n",
    "    ngram_range   = (1, 1),\n",
    "    max_features  = 40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "C_parameter = np.arange(0.1, 1, 0.1) \n",
    "\n",
    "# use best C\n",
    "\n",
    "C_parameter = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.3 fold 0 precision 0.815 recall 0.846 fscore 0.83\n",
      "---------------------------------------------\n",
      "FOLD 0: AUC PR-C = 0.843, AUC ROC = 0.952\n",
      "---------------------------------------------\n",
      "\n",
      " 0.3 fold 1 precision 0.769 recall 0.769 fscore 0.769\n",
      "---------------------------------------------\n",
      "FOLD 1: AUC PR-C = 0.89, AUC ROC = 0.97\n",
      "---------------------------------------------\n",
      "\n",
      " 0.3 fold 2 precision 0.828 recall 0.889 fscore 0.857\n",
      "---------------------------------------------\n",
      "FOLD 2: AUC PR-C = 0.918, AUC ROC = 0.978\n",
      "---------------------------------------------\n",
      "\n",
      " 0.3 fold 3 precision 0.677 recall 0.913 fscore 0.778\n",
      "---------------------------------------------\n",
      "FOLD 3: AUC PR-C = 0.872, AUC ROC = 0.959\n",
      "---------------------------------------------\n",
      "\n",
      " 0.3 fold 4 precision 0.811 recall 0.938 fscore 0.87\n",
      "---------------------------------------------\n",
      "FOLD 4: AUC PR-C = 0.886, AUC ROC = 0.969\n",
      "---------------------------------------------\n",
      "\n",
      "-----------------------------------------------\n",
      "CV average: AUC PR-C = 0.882, AUC ROC = 0.966, FSCORE  = 0.821\n",
      "-----------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c_p in C_parameter:  \n",
    "      for c, (train_index, val_index) in enumerate(kf.split(X_t, y_t)):\n",
    "\n",
    "            X_train, X_val      = X.iloc[train_index], X.iloc[val_index]\n",
    "            y_train, y_val      = y.iloc[train_index], y.iloc[val_index] \n",
    "            word_vectorizer.fit(X_train)\n",
    "            train_word_features = word_vectorizer.transform(X_train)\n",
    "            val_word_features   = word_vectorizer.transform(X_val)\n",
    "            y_train             = y_train.astype('int')\n",
    "            y_val               = y_val.astype('int')\n",
    "            classifier          = LogisticRegression(C=c_p, solver='sag')\n",
    "            classifier.fit(train_word_features, y_train)\n",
    "            probs               = classifier.predict_proba(val_word_features)[:,1]\n",
    "            auc_roc             = roc_auc_score(y_val, probs)\n",
    "            auc_pr              = average_precision_score(y_val, probs)\n",
    "            \n",
    "            threshold = 0.3\n",
    "            probs_class = probs.copy()\n",
    "            probs_class[probs_class >= threshold] = 1 \n",
    "            probs_class[probs_class < threshold] = 0\n",
    "            precision = precision_score(y_val, probs_class) \n",
    "            recall    = recall_score(y_val, probs_class)\n",
    "            fscore    = f1_score(y_val, probs_class)\n",
    "            print(f' {threshold} fold {c} precision {round(precision, 3)} recall {round(recall, 3)} fscore {round(fscore,3)}')\n",
    "\n",
    "            if len(C_parameter)==1:\n",
    "                # print performance\n",
    "                print(f'---------------------------------------------')\n",
    "                print(f'FOLD {c}: AUC PR-C = {round(auc_pr, 3)}, AUC ROC = {round(auc_roc, 3)}')\n",
    "                print(f'---------------------------------------------')\n",
    "                print(f'')\n",
    "\n",
    "            auc.append(auc_pr)\n",
    "            roc.append(auc_roc)\n",
    "            fscore_.append(fscore)\n",
    "\n",
    "      if len(C_parameter)!=1:\n",
    "             print(f'PARAMETER C = {c_p}')\n",
    "\n",
    "      # print performance\n",
    "      print(f'-----------------------------------------------')\n",
    "      print(f'CV average: AUC PR-C = {round(np.array(auc).mean(), 3)}, AUC ROC = {round(np.array(roc).mean(), 3)}, FSCORE  = {round(np.array(fscore_).mean(), 3)}')\n",
    "      print(f'-----------------------------------------------')\n",
    "      print(f'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "TEST: AUC PR-C = 0.9881, AUC ROC = 0.9956\n",
      "-----------------------------------------\n",
      "\n",
      " 0.3 precision 0.792 recall 1.0 fscore 0.884\n"
     ]
    }
   ],
   "source": [
    "word_vectorizer.fit(X)\n",
    "train_word_features  = word_vectorizer.transform(X)\n",
    "test_word_features   = word_vectorizer.transform(X_test)\n",
    "classifier           = LogisticRegression(C=1, solver='sag')\n",
    "classifier.fit(train_word_features, y)\n",
    "probs                = classifier.predict_proba(test_word_features)[:,1]\n",
    "auc_roc              = roc_auc_score(y_test, probs)\n",
    "auc_pr               = average_precision_score(y_test, probs)\n",
    "\n",
    "# print performance\n",
    "print(f'-----------------------------------------')\n",
    "print(f'TEST: AUC PR-C = {round(auc_pr, 4)}, AUC ROC = {round(auc_roc, 4)}')\n",
    "print(f'-----------------------------------------')\n",
    "print(f'')\n",
    "\n",
    "threshold = 0.3\n",
    "probs_class = probs.copy()\n",
    "probs_class[probs_class >= threshold] = 1 \n",
    "probs_class[probs_class < threshold] = 0\n",
    "precision = precision_score(y_test, probs_class) \n",
    "recall    = recall_score(y_test, probs_class)\n",
    "fscore    = f1_score(y_test, probs_class)\n",
    "print(f' {threshold} precision {round(precision, 3)} recall {round(recall, 3)} fscore {round(fscore,3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf  = StratifiedKFold(n_splits=5, random_state=42)\n",
    "auc = []\n",
    "roc = []\n",
    "fscore_ = []\n",
    "c   = 0\n",
    "\n",
    "word_vectorizer   = TfidfVectorizer(\n",
    "    sublinear_tf  = True,\n",
    "    strip_accents = 'unicode',\n",
    "    analyzer      = 'word',\n",
    "    token_pattern = r'\\w{1,}',\n",
    "    stop_words    = 'english',\n",
    "    ngram_range   = (1, 1),\n",
    "    max_features  = 40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend ThreadingBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  10 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=20)]: Done 160 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=20)]: Done 410 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=20)]: Done 600 out of 600 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=20)]: Using backend ThreadingBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  10 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=20)]: Done 160 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=20)]: Done 410 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=20)]: Done 600 out of 600 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "FOLD 0: AUC PR-C = 0.824, AUC ROC = 0.935\n",
      "---------------------------------------------\n",
      "\n",
      " 0.3 fold 0 precision 0.733 recall 0.846 fscore 0.786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend ThreadingBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  10 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=20)]: Done 160 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=20)]: Done 410 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=20)]: Done 600 out of 600 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=20)]: Using backend ThreadingBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  10 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=20)]: Done 160 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=20)]: Done 410 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=20)]: Done 600 out of 600 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "FOLD 1: AUC PR-C = 0.889, AUC ROC = 0.967\n",
      "---------------------------------------------\n",
      "\n",
      " 0.3 fold 1 precision 0.808 recall 0.808 fscore 0.808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend ThreadingBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  10 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=20)]: Done 160 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=20)]: Done 410 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=20)]: Done 600 out of 600 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=20)]: Using backend ThreadingBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  10 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=20)]: Done 160 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=20)]: Done 410 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=20)]: Done 600 out of 600 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "FOLD 2: AUC PR-C = 0.904, AUC ROC = 0.975\n",
      "---------------------------------------------\n",
      "\n",
      " 0.3 fold 2 precision 0.786 recall 0.815 fscore 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend ThreadingBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  10 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=20)]: Done 160 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=20)]: Done 410 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=20)]: Done 600 out of 600 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=20)]: Using backend ThreadingBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  10 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=20)]: Done 160 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=20)]: Done 410 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=20)]: Done 600 out of 600 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "FOLD 3: AUC PR-C = 0.882, AUC ROC = 0.955\n",
      "---------------------------------------------\n",
      "\n",
      " 0.3 fold 3 precision 0.656 recall 0.913 fscore 0.764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend ThreadingBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  10 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=20)]: Done 160 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=20)]: Done 410 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=20)]: Done 600 out of 600 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=20)]: Using backend ThreadingBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  10 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=20)]: Done 160 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=20)]: Done 410 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "FOLD 4: AUC PR-C = 0.922, AUC ROC = 0.977\n",
      "---------------------------------------------\n",
      "\n",
      " 0.3 fold 4 precision 0.795 recall 0.969 fscore 0.873\n",
      "-----------------------------------------------\n",
      "CV average: AUC PR-C = 0.884, AUC ROC = 0.962, FSCORE  = 0.806\n",
      "-----------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Done 600 out of 600 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "for c, (train_index, val_index) in enumerate(kf.split(X_t, y_t)):\n",
    "\n",
    "            X_train, X_val      = X.iloc[train_index], X.iloc[val_index]\n",
    "            y_train, y_val      = y.iloc[train_index], y.iloc[val_index] \n",
    "            word_vectorizer.fit(X_train)\n",
    "            train_word_features = word_vectorizer.transform(X_train)\n",
    "            val_word_features   = word_vectorizer.transform(X_val)\n",
    "            y_train             = y_train.astype('int')\n",
    "            y_val               = y_val.astype('int')\n",
    "            classifier          = RandomForestClassifier(n_estimators=600, max_depth=None, max_features='auto', \n",
    "                                    min_samples_split=2, verbose = True, n_jobs=20)\n",
    "            classifier.fit(train_word_features, y_train)\n",
    "            probs               = classifier.predict_proba(val_word_features)[:,1]\n",
    "            auc_roc             = roc_auc_score(y_val, probs)\n",
    "            auc_pr              = average_precision_score(y_val, probs)\n",
    "\n",
    "            \n",
    "            # print performance\n",
    "            print(f'---------------------------------------------')\n",
    "            print(f'FOLD {c}: AUC PR-C = {round(auc_pr, 3)}, AUC ROC = {round(auc_roc, 3)}')\n",
    "            print(f'---------------------------------------------')\n",
    "            print(f'')\n",
    "\n",
    "            auc.append(auc_pr)\n",
    "            roc.append(auc_roc)\n",
    "            \n",
    "            threshold = 0.3\n",
    "            probs_class = probs.copy()\n",
    "            probs_class[probs_class >= threshold] = 1 \n",
    "            probs_class[probs_class < threshold] = 0\n",
    "            precision = precision_score(y_val, probs_class) \n",
    "            recall    = recall_score(y_val, probs_class)\n",
    "            fscore    = f1_score(y_val, probs_class)\n",
    "            print(f' {threshold} fold {c} precision {round(precision, 3)} recall {round(recall, 3)} fscore {round(fscore,3)}')\n",
    "\n",
    "            fscore_.append(fscore)\n",
    "\n",
    "# print performance\n",
    "print(f'-----------------------------------------------')\n",
    "print(f'CV average: AUC PR-C = {round(np.array(auc).mean(), 3)}, AUC ROC = {round(np.array(roc).mean(), 3)}, FSCORE  = {round(np.array(fscore_).mean(), 3)}')\n",
    "print(f'-----------------------------------------------')\n",
    "print(f'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend ThreadingBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  10 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=20)]: Done 160 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=20)]: Done 410 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=20)]: Done 600 out of 600 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=20)]: Using backend ThreadingBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  10 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=20)]: Done 160 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "TEST: AUC PR-C = 1.0, AUC ROC = 1.0\n",
      "-----------------------------------------\n",
      "\n",
      " 0.3 precision 1.0 recall 1.0 fscore 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Done 410 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=20)]: Done 600 out of 600 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "# TRAIN ON WHOLE DAATA AND PREDICT ON TEST\n",
    "word_vectorizer.fit(X)\n",
    "train_word_features  = word_vectorizer.transform(X)\n",
    "test_word_features   = word_vectorizer.transform(X_test)\n",
    "classifier           = RandomForestClassifier(n_estimators=600, max_depth=None, max_features='auto', \n",
    "                                    min_samples_split=2, verbose = True, n_jobs=20)\n",
    "classifier.fit(train_word_features, y)\n",
    "probs                = classifier.predict_proba(test_word_features)[:,1]\n",
    "auc_roc              = roc_auc_score(y_test, probs)\n",
    "auc_pr               = average_precision_score(y_test, probs)\n",
    "\n",
    "# print performance\n",
    "print(f'-----------------------------------------')\n",
    "print(f'TEST: AUC PR-C = {round(auc_pr, 3)}, AUC ROC = {round(auc_roc, 3)}')\n",
    "print(f'-----------------------------------------')\n",
    "print(f'')\n",
    "\n",
    "threshold = 0.3\n",
    "probs_class = probs.copy()\n",
    "probs_class[probs_class >= threshold] = 1 \n",
    "probs_class[probs_class < threshold] = 0\n",
    "precision = precision_score(y_test, probs_class) \n",
    "recall    = recall_score(y_test, probs_class)\n",
    "fscore    = f1_score(y_test, probs_class)\n",
    "print(f' {threshold} precision {round(precision, 3)} recall {round(recall, 3)} fscore {round(fscore,3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "penalty l1\n",
      "alpha 1e-05\n",
      "max_iter 1000\n",
      "-------------\n",
      "---------------------------------------------\n",
      "FOLD 0: AUC PR-C = 0.88, AUC ROC = 0.963\n",
      "---------------------------------------------\n",
      "\n",
      " 0.3 fold 0 precision 0.818 recall 0.692 fscore 0.75\n",
      "---------------------------------------------\n",
      "FOLD 1: AUC PR-C = 0.934, AUC ROC = 0.982\n",
      "---------------------------------------------\n",
      "\n",
      " 0.3 fold 1 precision 0.87 recall 0.769 fscore 0.816\n",
      "---------------------------------------------\n",
      "FOLD 2: AUC PR-C = 0.919, AUC ROC = 0.977\n",
      "---------------------------------------------\n",
      "\n",
      " 0.3 fold 2 precision 0.852 recall 0.852 fscore 0.852\n",
      "---------------------------------------------\n",
      "FOLD 3: AUC PR-C = 0.932, AUC ROC = 0.975\n",
      "---------------------------------------------\n",
      "\n",
      " 0.3 fold 3 precision 0.792 recall 0.826 fscore 0.809\n",
      "---------------------------------------------\n",
      "FOLD 4: AUC PR-C = 0.898, AUC ROC = 0.972\n",
      "---------------------------------------------\n",
      "\n",
      " 0.3 fold 4 precision 0.775 recall 0.969 fscore 0.861\n",
      "-----------------------------------------------\n",
      "CV average: AUC PR-C = 0.913, AUC ROC = 0.974, FSCORE  = 0.818\n",
      "-----------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "penalty = ['l2', 'l1']\n",
    "alpha = [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1]\n",
    "max_iter = [1000, 10000, 15000]\n",
    "\n",
    "# best parameters\n",
    "penalty = ['l1']\n",
    "alpha = [0.00001]\n",
    "max_iter = [1000] \n",
    "\n",
    "for p in penalty:\n",
    "  for a in alpha:\n",
    "    for i in max_iter:\n",
    "      auc = []\n",
    "      roc = []\n",
    "      fscore_ = []\n",
    "      c = 0\n",
    "      # print performance\n",
    "      print(f'-------------')\n",
    "      print(f'penalty {p}')\n",
    "      print(f'alpha {a}')\n",
    "      print(f'max_iter {i}')\n",
    "      print(f'-------------')\n",
    "      \n",
    "      for c, (train_index, val_index) in enumerate(kf.split(X_t, y_t)):\n",
    "\n",
    "            X_train, X_val      = X.iloc[train_index], X.iloc[val_index]\n",
    "            y_train, y_val      = y.iloc[train_index], y.iloc[val_index] \n",
    "            word_vectorizer.fit(X_train)\n",
    "            train_word_features = word_vectorizer.transform(X_train)\n",
    "            val_word_features   = word_vectorizer.transform(X_val)\n",
    "            y_train             = y_train.astype('int')\n",
    "            y_val               = y_val.astype('int')\n",
    "            classifier          = SGDClassifier(n_jobs=20, random_state=RS, loss='log', shuffle=False, \n",
    "                                    penalty=p, alpha=a, max_iter=i)\n",
    "            classifier.fit(train_word_features, y_train)\n",
    "            probs               = classifier.predict_proba(val_word_features)[:,1]\n",
    "            auc_roc             = roc_auc_score(y_val, probs)\n",
    "            auc_pr              = average_precision_score(y_val, probs)\n",
    "            \n",
    "            \n",
    "            print(f'---------------------------------------------')\n",
    "            print(f'FOLD {c}: AUC PR-C = {round(auc_pr, 3)}, AUC ROC = {round(auc_roc, 3)}')\n",
    "            print(f'---------------------------------------------')\n",
    "            print(f'')\n",
    "\n",
    "            auc.append(auc_pr)\n",
    "            roc.append(auc_roc)\n",
    "            threshold = 0.3\n",
    "            probs_class = probs.copy()\n",
    "            probs_class[probs_class >= threshold] = 1 \n",
    "            probs_class[probs_class < threshold] = 0\n",
    "            precision = precision_score(y_val, probs_class) \n",
    "            recall    = recall_score(y_val, probs_class)\n",
    "            fscore    = f1_score(y_val, probs_class)\n",
    "            print(f' {threshold} fold {c} precision {round(precision, 3)} recall {round(recall, 3)} fscore {round(fscore,3)}')\n",
    "\n",
    "            fscore_.append(fscore)\n",
    "\n",
    "# print performance\n",
    "print(f'-----------------------------------------------')\n",
    "print(f'CV average: AUC PR-C = {round(np.array(auc).mean(), 3)}, AUC ROC = {round(np.array(roc).mean(), 3)}, FSCORE  = {round(np.array(fscore_).mean(), 3)}')\n",
    "print(f'-----------------------------------------------')\n",
    "print(f'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "TEST: AUC PR-C = 1.0, AUC ROC = 1.0\n",
      "-----------------------------------------\n",
      "\n",
      " 0.3 precision 1.0 recall 1.0 fscore 1.0\n"
     ]
    }
   ],
   "source": [
    "word_vectorizer.fit(X)\n",
    "train_word_features  = word_vectorizer.transform(X)\n",
    "test_word_features   = word_vectorizer.transform(X_test)\n",
    "classifier           = SGDClassifier(n_jobs=20, random_state=RS, loss='log', shuffle=False, \n",
    "                                    penalty=penalty[0], alpha=alpha[0], max_iter=max_iter[0])\n",
    "classifier.fit(train_word_features, y)\n",
    "probs                = classifier.predict_proba(test_word_features)[:,1]\n",
    "auc_roc              = roc_auc_score(y_test, probs)\n",
    "auc_pr               = average_precision_score(y_test, probs)\n",
    "\n",
    "# print performance\n",
    "print(f'-----------------------------------------')\n",
    "print(f'TEST: AUC PR-C = {round(auc_pr, 3)}, AUC ROC = {round(auc_roc, 3)}')\n",
    "print(f'-----------------------------------------')\n",
    "print(f'')\n",
    "\n",
    "threshold = 0.3\n",
    "probs_class = probs.copy()\n",
    "probs_class[probs_class >= threshold] = 1 \n",
    "probs_class[probs_class < threshold] = 0\n",
    "precision = precision_score(y_test, probs_class) \n",
    "recall    = recall_score(y_test, probs_class)\n",
    "fscore    = f1_score(y_test, probs_class)\n",
    "print(f' {threshold} precision {round(precision, 3)} recall {round(recall, 3)} fscore {round(fscore,3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
